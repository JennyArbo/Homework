{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "diabetes_df = pd.read_csv(\"diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "# #Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8514, -0.9801, -0.4048,  ..., -0.6077,  0.3108, -0.7922],\n",
      "        [ 0.3566,  0.1614,  0.4654,  ..., -0.3021, -0.1164,  0.5610],\n",
      "        [-0.5494, -0.5045, -0.6223,  ...,  0.3726, -0.7649, -0.7076],\n",
      "        ...,\n",
      "        [-0.8514, -0.7582,  0.0303,  ...,  0.7800, -0.7861, -0.2847],\n",
      "        [ 1.8665, -0.3142,  0.0303,  ..., -0.5695, -1.0194,  0.5610],\n",
      "        [ 0.0546,  0.7322, -0.6223,  ..., -0.3149, -0.5770,  0.3073]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #this has activation functions\n",
    "\n",
    "# Creating tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, out_features =2):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "#optimizer #2: SGD:\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "#optimizer #3: SGD without momentum specified:\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 0.6611752510070801\n",
      "Epoch number: 11 with loss: 0.659355103969574\n",
      "Epoch number: 21 with loss: 0.657654881477356\n",
      "Epoch number: 31 with loss: 0.6560633182525635\n",
      "Epoch number: 41 with loss: 0.6545655727386475\n",
      "Epoch number: 51 with loss: 0.6531487107276917\n",
      "Epoch number: 61 with loss: 0.6517978310585022\n",
      "Epoch number: 71 with loss: 0.6505092978477478\n",
      "Epoch number: 81 with loss: 0.6492682695388794\n",
      "Epoch number: 91 with loss: 0.6480698585510254\n",
      "Epoch number: 101 with loss: 0.6468980312347412\n",
      "Epoch number: 111 with loss: 0.6457501649856567\n",
      "Epoch number: 121 with loss: 0.6446250677108765\n",
      "Epoch number: 131 with loss: 0.6435220241546631\n",
      "Epoch number: 141 with loss: 0.642434298992157\n",
      "Epoch number: 151 with loss: 0.6413562893867493\n",
      "Epoch number: 161 with loss: 0.6402857899665833\n",
      "Epoch number: 171 with loss: 0.6392180919647217\n",
      "Epoch number: 181 with loss: 0.6381511688232422\n",
      "Epoch number: 191 with loss: 0.6370807886123657\n",
      "Epoch number: 201 with loss: 0.6360084414482117\n",
      "Epoch number: 211 with loss: 0.6349335312843323\n",
      "Epoch number: 221 with loss: 0.633854866027832\n",
      "Epoch number: 231 with loss: 0.6327680945396423\n",
      "Epoch number: 241 with loss: 0.6316660642623901\n",
      "Epoch number: 251 with loss: 0.6305500864982605\n",
      "Epoch number: 261 with loss: 0.6294218897819519\n",
      "Epoch number: 271 with loss: 0.6282811164855957\n",
      "Epoch number: 281 with loss: 0.6271247267723083\n",
      "Epoch number: 291 with loss: 0.6259581446647644\n",
      "Epoch number: 301 with loss: 0.6247708797454834\n",
      "Epoch number: 311 with loss: 0.6235657334327698\n",
      "Epoch number: 321 with loss: 0.6223405599594116\n",
      "Epoch number: 331 with loss: 0.6210905313491821\n",
      "Epoch number: 341 with loss: 0.619817852973938\n",
      "Epoch number: 351 with loss: 0.618527889251709\n",
      "Epoch number: 361 with loss: 0.6172171235084534\n",
      "Epoch number: 371 with loss: 0.6158809661865234\n",
      "Epoch number: 381 with loss: 0.6145267486572266\n",
      "Epoch number: 391 with loss: 0.6131519079208374\n",
      "Epoch number: 401 with loss: 0.6117507815361023\n",
      "Epoch number: 411 with loss: 0.6103302240371704\n",
      "Epoch number: 421 with loss: 0.6088908910751343\n",
      "Epoch number: 431 with loss: 0.6074312329292297\n",
      "Epoch number: 441 with loss: 0.6059442162513733\n",
      "Epoch number: 451 with loss: 0.6044321060180664\n",
      "Epoch number: 461 with loss: 0.6029012799263\n",
      "Epoch number: 471 with loss: 0.6013456583023071\n",
      "Epoch number: 481 with loss: 0.5997722744941711\n",
      "Epoch number: 491 with loss: 0.5981811881065369\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = model(data)\n",
    "        y_pred.append(prediction.argmax().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6493506493506493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a_score = accuracy_score(y_test, y_pred)\n",
    "print(a_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79       100\n",
      "           1       0.00      0.00      0.00        54\n",
      "\n",
      "    accuracy                           0.65       154\n",
      "   macro avg       0.32      0.50      0.39       154\n",
      "weighted avg       0.42      0.65      0.51       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferarbuszewski/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jenniferarbuszewski/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jenniferarbuszewski/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Class Exercise Starts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look up the Adam optimization functions in PyTorch https://pytorch.org/docs/stable/optim.html . How does it work? Try at least one other optimization function with the diabetes dataset shown in class. How does the model perform with the new optimizer? Did it perform better or worse than Adam? Why do you think that is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of all optimizers in PyTorch is to minimize the loss function (error) of a model during the training phases. The loss function is minimized through changing the weights assigned at each step of the process. The way this is accomplished varies dependent upon the specific optimizer.\n",
    "\n",
    "The Adam optimization function works to find the lowest (optimized) values for its loss functions. Adam does this through adjusting the learning rate (a model parameter) throughout the model iterations instead of keeping it as a constant value. It also takes advantage of momentum. The advantage of using momentum is that the optimization will continue even if the model encounters a low point in the loss function value. This enables the model to continue to the \"true\" minimum value of the loss function. Adam is a very popular and commonly used optimizer.\n",
    "\n",
    "I decided to also apply the SGD (Stochastic Gradient) optimizer because it is one of the most commonly used optimizers currently available. As a note, Adam and SGD are based on some of the same principles so they may not be as different as some other optimizers available. \n",
    "\n",
    "|      Optimizer     |  accuracy     |         precision       |       recall     |\n",
    "|--------------------|---------------|-------------------------|------------------|\n",
    "|       Adam:        |    0.6948     |            0.70         |         0.69     |\n",
    "|        SGD:        |    0.7273     |            0.73         |         0.73     |\n",
    "|  SGD (no momentum) |    0.6494     |            0.42         |         0.65     |\n",
    "\n",
    "In the end, the SGD optimizer performed slightly better for this dataset in all metrics (accuracy, precision, and recall). Of particular note, the optimized SGD performance occurred when momentum was specified for the optimizer. Without the use of momentum for SGD (so just specifying the learning rate parameter), performance dropped significantly for all 3 metrics and most precipitously for precision. Basically, you need to make sure you optimize your optimizer and when comparing optimizers make sure you compare apples to apples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Write a function that lists and counts the number of divisors for an input value. Example 1: Input: 5 Output: “There are 2 divisors: 1 and 5” Example 2: Input: 40 Output: “There are 8 divisors: 1, 2, 4, 5, 8, 10, 20, and 40”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  8  divisors: 1, 2, 4, 5, 8, 10, 20 and 40\n",
      "There are  1  divisors: 1\n",
      "There are  2  divisors: 1 and 2\n"
     ]
    }
   ],
   "source": [
    "def divisors(num):\n",
    "    divisors_list = []\n",
    "    for n in range (1, num + 1):\n",
    "        if num % n == 0:\n",
    "            divisors_list.append(n)\n",
    "   \n",
    "    number_divisors = len(divisors_list)\n",
    "    divisors_string = [str(element) for element in divisors_list]\n",
    "    divisors_string_list = \" and \".join([\", \".join(divisors_string[:-1]), divisors_string[-1]] \n",
    "                                       if len(divisors_string)>2 else divisors_string)\n",
    "    return(print(\"There are \", number_divisors, \" divisors:\", divisors_string_list ))\n",
    "\n",
    "#Test Cases:\n",
    "divisors(40)\n",
    "divisors(1)\n",
    "divisors(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
