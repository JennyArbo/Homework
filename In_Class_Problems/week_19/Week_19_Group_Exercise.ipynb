{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 19 Group Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Take one of the supervised learning models you have built recently and apply at least three dimensionality reduction techniques to it (separately). Be sure to create a short summary of each technique you use. Indicate how each changed the model performance. Reference: https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short description of why dimensionality reduction techniques are useful:**\n",
    "\n",
    "Dataset containing high dimensional data can be difficult to handle. In particular, high dimensions are difficult to visualize and can be hard to draw conclusions from or identify patterns in. Using dimensionality reduction can help address those problems and can also help decrease storage needs, decrease computational time, may lead to enhanced algorithm perfomance, and can decrease or eliminate issues caused by multicolinearity within a dataset. The primary goal of any dimensionality reduction technique is to maintain the most important information in the dataet/its basic structure while refitting the data in a transformation to a lower dimension. There are techniques which both linearly and non-linearly transform the data. Here, I look at examples of each class. However, as I observed in this exercise, not every dataset needs or benefits from these techniques. It is again a case of knowing your data, experimenting and applying applicable techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_wt</th>\n",
       "      <th>shucked_wt</th>\n",
       "      <th>viscera_wt</th>\n",
       "      <th>shell_wt</th>\n",
       "      <th>rings</th>\n",
       "      <th>age</th>\n",
       "      <th>age_sex</th>\n",
       "      <th>age_sex_A</th>\n",
       "      <th>age_sex_I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>16.5</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>10.5</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>11.5</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  diameter  height  whole_wt  shucked_wt  viscera_wt  shell_wt  \\\n",
       "0   0.455     0.365   0.095    0.5140      0.2245      0.1010     0.150   \n",
       "1   0.350     0.265   0.090    0.2255      0.0995      0.0485     0.070   \n",
       "2   0.530     0.420   0.135    0.6770      0.2565      0.1415     0.210   \n",
       "3   0.440     0.365   0.125    0.5160      0.2155      0.1140     0.155   \n",
       "4   0.330     0.255   0.080    0.2050      0.0895      0.0395     0.055   \n",
       "\n",
       "   rings   age age_sex  age_sex_A  age_sex_I  \n",
       "0     15  16.5       A          1          0  \n",
       "1      7   8.5       A          1          0  \n",
       "2      9  10.5       A          1          0  \n",
       "3     10  11.5       A          1          0  \n",
       "4      7   8.5       I          0          1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the usual things\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#reading in the csv file for the dataset I want as a pandas dataframe\n",
    "df = pd.read_csv(r'outlier_removed_abalone.csv')\n",
    "\n",
    "#viewing the head of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I decided to use a simple linear regression as my model of interest for this problem. I chose a linear regression because it achieved results similar to more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "\n",
    "#### Linear Method: Principal Components Analysis (PCA)\n",
    "    \n",
    "PCA is an extremely popular method of dimensionality reduction although it should typically only be used with dense data (very few missing data points). Since the dataset I chose has no missing values (all were removed prior to the dataset being posted to the UCI data repository), PCA may be a good fit. We also know from past analyses that there are multiple variables that have colinearity with one another. \n",
    "\n",
    "The primary goal of PCA is to capture as much data variance as possible in as few Principal Components as possible, thereby reducing the number of features used and decreasing model complexity. PCA typically involves standardizing the data (it is sensitive to large differences in variance), creating a covariance matrix and using the eigenvectors and eigenvalues to identify the linear combinations which then form the principal components. The new principal components are not correlated and typically, the very first few principal components will contain the vast majority of the sample variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#defining features and target as X and y\n",
    "X = df.drop(['age', 'rings', 'age_sex'], axis=1)\n",
    "\n",
    "y = df['age']\n",
    "\n",
    "\n",
    "#using standard scaler to scale the X variables\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multivariate Linear Model on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                    age   R-squared (uncentered):                   0.029\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.027\n",
      "Method:                 Least Squares   F-statistic:                              11.73\n",
      "Date:                Fri, 09 Jul 2021   Prob (F-statistic):                    1.40e-16\n",
      "Time:                        01:05:40   Log-Likelihood:                         -12049.\n",
      "No. Observations:                3135   AIC:                                  2.411e+04\n",
      "Df Residuals:                    3127   BIC:                                  2.416e+04\n",
      "Df Model:                           8                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.2749      1.265     -0.217      0.828      -2.755       2.205\n",
      "x2             0.9073      1.274      0.712      0.476      -1.590       3.404\n",
      "x3             0.6465      0.518      1.247      0.212      -0.370       1.663\n",
      "x4             2.5371      2.049      1.238      0.216      -1.480       6.554\n",
      "x5            -3.0055      1.060     -2.834      0.005      -5.085      -0.926\n",
      "x6            -0.6272      0.839     -0.748      0.455      -2.272       1.018\n",
      "x7             1.1700      0.948      1.234      0.217      -0.689       3.029\n",
      "x8             0.1859      0.125      1.490      0.136      -0.059       0.431\n",
      "x9            -0.1859      0.125     -1.490      0.136      -0.431       0.059\n",
      "==============================================================================\n",
      "Omnibus:                      361.866   Durbin-Watson:                   0.049\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              558.553\n",
      "Skew:                           0.829   Prob(JB):                    5.15e-122\n",
      "Kurtosis:                       4.235   Cond. No.                     1.26e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[3] The smallest eigenvalue is 1.43e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#from sklearn import linear_model\n",
    "#linear_reg = linear_model.LinearRegression()\n",
    "#linear_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#how/can I use ols with a training and test set? I haven't been able to figure that out yet and would like \n",
    "#to use it over sklearn's linear_model since it was presented as being the \"gold standard\"\n",
    "\n",
    "\n",
    "\n",
    "#print('r^2:', T_multi1.rsquared)\n",
    "#print('r^2 adjusted:', T_multi1.rsquared_adj)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "linear_model = sm.OLS(y_train, X_train)\n",
    "results = linear_model.fit()\n",
    "print(results.summary())\n",
    "\n",
    "#Not sure why I'm getting results that seem so far off from the other Linear Regression model I used..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  3.634673828159976\n",
      "RMSE:  1.9064820555567723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "print('MSE: ', MSE)\n",
    "print('RMSE: ', np.sqrt(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA and determining the number of PCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative variance explained for PCA of abalone dataset')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Trying to figure out an optimal number of principal components to use for my analysis\n",
    "\n",
    "#fitting the PCA to my training dataset (I also tried on the whole dataset and achieved the same answers,\n",
    "#as I believe should be expected)\n",
    "pca = PCA().fit(X_train)\n",
    "\n",
    "#Plotting the explained variance for 8 PCs\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number components')\n",
    "plt.ylabel('Cumulative variance explained for PCA of abalone dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As can be seen by the plot above, we get up to 95% variance explained using only 1 PC. By the time we get to 3 PC, over 97.5% of the variance can be explained. I decided to use 3 PC for my model input moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81136214 0.13489789 0.02124357]\n"
     ]
    }
   ],
   "source": [
    "pca_final = PCA(n_components = 3)\n",
    " \n",
    "X_train_pca = pca_final.fit_transform(X_train)\n",
    "X_test_pca = pca_final.transform(X_test)\n",
    " \n",
    "explained_variance = pca_final.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the model to my new X data that contains the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  3.974599153786927\n",
      "RMSE:  1.9936396750132475\n"
     ]
    }
   ],
   "source": [
    "lin_reg_pca = LinearRegression()\n",
    "\n",
    "lin_reg_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = lin_reg_pca.predict(X_test_pca)\n",
    "\n",
    "from sklearn import metrics\n",
    "MSE_pca = metrics.mean_squared_error(y_test, y_pred_pca)\n",
    "print('MSE: ', MSE_pca)\n",
    "print('RMSE: ', np.sqrt(MSE_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying PCA to my data actually resulted in a decrease in my model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "##### Linear Method: Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is a linear dimensionality reduction technique. It relies heavily on linear algebra. Like PCA, SVD is performed with a goal of reducing the number of features while preserving the most important components of a dataset (in other words, preserving maximum variance). SVD is based on the concept that a single matrix can be made which holds the same basic data structure/information as several other matrices. By using a few key characteristics of the single matrix, we can approximate the original data using fewer dimensions.  Unlike PCA, SVD is a popular technique when there are many missing values (sparse data). For dense data, PCA may often be preferred for a dataset like mine where there are few missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  4.598023477177701\n",
      "RMSE:  2.144300230186459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#Fitting the SVD class\n",
    "svd =  TruncatedSVD(n_components = 2)\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_test_svd = svd.fit_transform(X_test)\n",
    "\n",
    "#Instantiating the model\n",
    "lin_reg_svd = LinearRegression()\n",
    "\n",
    "#Fitting the model on the X_train data that has had SVD applied and my original train data\n",
    "lin_reg_svd.fit(X_train_svd, y_train)\n",
    "\n",
    "#Using the SVD data to predict y values\n",
    "y_pred_svd = lin_reg_svd.predict(X_test_svd)\n",
    "\n",
    "\n",
    "#Calculating performance metrics for my model\n",
    "MSE_svd = metrics.mean_squared_error(y_test, y_pred_svd)\n",
    "print('MSE: ', MSE_svd)\n",
    "print('RMSE: ', np.sqrt(MSE_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As anticipated, the PCA dimensionality reduction outperformed SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 & 4\n",
    "##### Multidimensional Scaling: Local Linear Embedding & Isomap Embedding\n",
    "\n",
    "While PCA is often a useful technique, it is limited by the requirement that a linear model is used and is therefore not useful for all datasets. PCA is especially sensitive to some qualities of a dataset such as a highly skewed distribution, extreme outliers, and even one-hot encoded data that has a large number of classes/variables encoded. This is where multidimensional scaling techniques can come in.\n",
    "\n",
    "A manifold is a smooth, lower dimensional object that can be exist/be projected onto a higher dimensional plane. For example, we could consider a piece of paper (2D) rolled into a spiral. The paper retains its 2D qualities (for example, length and width) even when it is existing as a 3D shape/embedded in a 3D field. If we can find a manifold that fits our data, we can use that as a way to decrease the number of dimensions (features) we need to describe the data. The relationship of the manifold to higher dimensional space is often, but not always (see MDS) non-linear. The 2 techniques I'm discussing here both rely on nearest neighbor techniques and the first step in each involves looking for and identifying nearest neighbors\n",
    "\n",
    "In local linear embedding, the data is divided up into \"neighborhoods\". Each local neighborhood essentially has a PCA applied to it. These PCAs are then compared to find the series of PCAs that best fits the data, thus preserving much of the original data structure. \n",
    "\n",
    "Isomap embedding is a multimensional scaling technique. It is a nearest neighbor technique and estimates each the features of a manifold based upon each data point's neighbors on that manifold. In isomap embeddingm the main goal of the algorithm is to preserve the geodesic(shortest line between 2 points on any curved surface) distances that exist in the original/untransformed dataset. Of note, this technique may be best used when combined with several runs of the algorithm-by nature, each time it is run, results vary to some degree. So, it is best to average several attempts to get an idea of the mean performance. I did not do this here due to time constraints before the exercise deadline, but a simple for loop should get the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  24.460173931879606\n",
      "RMSE:  4.94572279165337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "embed = LocallyLinearEmbedding(n_components=2) \n",
    "X_train_IE = embed.fit_transform(X_train)\n",
    "X_test_IE = embed.fit_transform(X_test)\n",
    "\n",
    "lin_reg_IE = LinearRegression()\n",
    "\n",
    "lin_reg_IE.fit(X_train_IE, y_train)\n",
    "y_pred_IE = lin_reg_IE.predict(X_test_IE)\n",
    "\n",
    "from sklearn import metrics\n",
    "MSE_IE = metrics.mean_squared_error(y_test, y_pred_IE)\n",
    "print('MSE: ', MSE_IE)\n",
    "print('RMSE: ', np.sqrt(MSE_IE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  5.54123970899882\n",
      "RMSE:  2.35398379539852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "iso_model = Isomap(n_components=3)\n",
    "X_train_iso = iso_model.fit_transform(X_train)\n",
    "X_test_iso = iso_model.fit_transform(X_test)\n",
    "\n",
    "lin_reg_iso = LinearRegression()\n",
    "\n",
    "lin_reg_iso.fit(X_train_iso, y_train)\n",
    "y_pred_iso = lin_reg_iso.predict(X_test_iso)\n",
    "\n",
    "from sklearn import metrics\n",
    "MSE_iso = metrics.mean_squared_error(y_test, y_pred_iso)\n",
    "print('MSE: ', MSE_iso)\n",
    "print('RMSE: ', np.sqrt(MSE_iso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Both of these techniques yielded poor model performance and both were considerably worse than using the original untransformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus technique: Multidimensional scaling (MDS)\n",
    "\n",
    "MDS relies on considering how similar or dissimilar datapoints are. It does this through evaluation of distance between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-11b20e611ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_mds.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, init)\u001b[0m\n\u001b[1;32m    452\u001b[0m                              \" Got %s instead\" % str(self.dissimilarity))\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         self.embedding_, self.stress_, self.n_iter_ = smacof(\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdissimilarity_matrix_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_mds.py\u001b[0m in \u001b[0;36msmacof\u001b[0;34m(dissimilarities, metric, n_components, init, n_init, n_jobs, max_iter, verbose, eps, random_state, return_n_iter)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             pos, stress, n_iter_ = _smacof_single(\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0mdissimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_mds.py\u001b[0m in \u001b[0;36m_smacof_single\u001b[0;34m(dissimilarities, metric, n_components, init, max_iter, verbose, eps, random_state)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Compute distance and monotonic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#MDS\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=3, random_state=42)\n",
    "X_train_mds = mds.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDS is considerably more computationally expensive than the other methods. In fact, it crashed my kernel and made my computer extremely unhappy so I did not try to compute it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data Transformation Technique | RMSE for Linear Model |\n",
    "|-------------------------------|-----------------------|\n",
    "| no transformation | 1.9065 |\n",
    "| PCA | 1.9936 |\n",
    "| SVD | 2.1443 |\n",
    "| LLE | 5.1447 |\n",
    "| Isomap | 2.3540 |\n",
    "| MDS | crashed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result involved no data transformation for this dataset. PCA transformation resulted in marginally lower performance but all other techniques results in significant losses in model performance using the transformed data. This suggests to me that the dataset I chose (abalone) can be described by simple, linear techniques. Some features may be combined or decreased (for instance, I know the weight features are all highly correlated) but in this case, the model performance is not increased by doing so. In higher dimension, more complex, or non-linearly related data, these approaches are likely to be more helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Write a function that will indicate if an inputted IPv4 address is accurate or not. IP addresses are valid if they have 4 values between 0 and 255 (inclusive), punctuated by periods.\n",
    "\n",
    "Input 1: 2.33.245.5 Output 1: True \n",
    "\n",
    "Input 2: 12.345.67.89 Output 2: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining my function\n",
    "def address_check(IP_input):\n",
    "    #Checking to make sure there are 4 values. If not, we can immediately return False\n",
    "    if str(IP_input).count('.') != 3:\n",
    "        return False\n",
    "    else:\n",
    "        #Splitting the IPv4 address into a list of components\n",
    "        split_list = IP_input.split(\".\")\n",
    "        #print(split_list)\n",
    "        for i in split_list:\n",
    "            i_int = int(i)\n",
    "            #print(i_int)\n",
    "            #Setting conditions given in the original question; if conditions are not met, return False\n",
    "            if 0 > i_int or i_int > 256:\n",
    "                return False\n",
    "        #If the IPv4 address passes everything else, we can confidently return True and say it is valid\n",
    "        return True\n",
    "\n",
    "#Test Cases:\n",
    "\n",
    "#address_check('2.33.245')\n",
    "#address_check('2.33.265.2')\n",
    "#address_check('2.33.245.5')\n",
    "address_check('12.345.67.89')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
